{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"Rank.csv\"  # adjust if needed\n",
    "\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "df[\"UpdateDT\"] = pd.to_datetime(df[\"UpdateDT\"], errors='coerce')\n",
    "df.sort_values([\"SubjectID\", \"UpdateDT\"], inplace=True)\n",
    "\n",
    "print(\"Data shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nInfo:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nDescribe:\")\n",
    "display(df.describe(include=\"all\"))\n",
    "\n",
    "sns.histplot(data=df, x=\"Rank\", kde=True, bins=30)\n",
    "plt.title(\"Distribution of Rank (Continuous Score)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns that start with 'Driver'\n",
    "driver_cols = [c for c in df.columns if c.startswith(\"Driver\")] \n",
    "\n",
    "# Convert nominal driver columns to dummies (drop_first=True to avoid dummy trap)\n",
    "df_encoded = pd.get_dummies(df, columns=driver_cols, drop_first=True)\n",
    "\n",
    "print(\"Shape before encoding:\", df.shape)\n",
    "print(\"Shape after encoding:\", df_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to incorporate time-based info (like days from earliest), do so here.\n",
    "\n",
    "if \"UpdateDT\" in df_encoded.columns:\n",
    "    earliest_date = df_encoded[\"UpdateDT\"].min()\n",
    "    df_encoded[\"DaysFromEarliest\"] = (df_encoded[\"UpdateDT\"] - earliest_date).dt.days\n",
    "\n",
    "    # Example: difference in days between consecutive rows of same SubjectID\n",
    "    df_encoded[\"TimeDiff\"] = df_encoded.groupby(\"SubjectID\")[\"UpdateDT\"].diff().dt.days\n",
    "    df_encoded[\"TimeDiff\"] = df_encoded[\"TimeDiff\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decide which columns to use for clustering. Typically exclude ID & UpdateDT.\n",
    "\n",
    "exclude_cols = [\"SubjectID\", \"UpdateDT\"]  # won't cluster on ID or actual date\n",
    "\n",
    "# Option A: Include \"Rank\" as a feature\n",
    "# Option B: Exclude \"Rank\" to see how clusters differ in rank after the fact.\n",
    "include_rank = True\n",
    "\n",
    "if include_rank:\n",
    "    exclude_cols.extend([])  # nothing extra to exclude\n",
    "else:\n",
    "    exclude_cols.append(\"Rank\")  # if we don't want to cluster on rank\n",
    "\n",
    "feature_cols = [c for c in df_encoded.columns if c not in exclude_cols]\n",
    "\n",
    "X = df_encoded[feature_cols].copy()\n",
    "\n",
    "# Check for missing\n",
    "missing = X.isnull().sum()\n",
    "print(\"\\nMissing values in features:\")\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# We'll fill or drop them if needed. For now, assume minimal missing.\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Feature matrix shape:\", X_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_values = range(2, 8)\n",
    "\n",
    "inertias = []\n",
    "sils = []\n",
    "for k in k_values:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_temp.fit(X_scaled)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    sils.append(silhouette_score(X_scaled, kmeans_temp.labels_))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(k_values, inertias, marker='o')\n",
    "ax1.set_title(\"Elbow Method (Inertia)\")\n",
    "ax1.set_xlabel(\"k\")\n",
    "ax1.set_ylabel(\"Inertia\")\n",
    "\n",
    "ax2.plot(k_values, sils, marker='o')\n",
    "ax2.set_title(\"Silhouette Scores\")\n",
    "ax2.set_xlabel(\"k\")\n",
    "ax2.set_ylabel(\"Score\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 4  # pick from the elbow/silhouette results\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "print(\"Cluster label counts:\")\n",
    "print(pd.Series(cluster_labels).value_counts())\n",
    "\n",
    "# Attach labels back to df_encoded for interpretation\n",
    "clustered_df = df_encoded.copy()\n",
    "clustered_df[\"Cluster\"] = cluster_labels\n",
    "\n",
    "print(\"\\nMean of 'Rank' by cluster (if included in features):\")\n",
    "if \"Rank\" in clustered_df.columns:\n",
    "    print(clustered_df.groupby(\"Cluster\")[\"Rank\"].mean())\n",
    "\n",
    "# We can also look at means of any driver dummy columns\n",
    "print(\"\\nSample driver factor frequencies by cluster:\")\n",
    "driver_dummy_cols = [c for c in clustered_df.columns if c.startswith(\"Driver\")] # after encoding\n",
    "means_by_cluster = clustered_df.groupby(\"Cluster\")[driver_dummy_cols].mean()\n",
    "display(means_by_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_result = pca.fit_transform(X_scaled)\n",
    "clustered_df[\"pca_1\"] = pca_result[:, 0]\n",
    "clustered_df[\"pca_2\"] = pca_result[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=clustered_df,\n",
    "    x=\"pca_1\", y=\"pca_2\",\n",
    "    hue=\"Cluster\",\n",
    "    palette=\"Set1\",\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"K-Means Clusters (PCA=2 components)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "In this section, we demonstrate how to train a predictive model (e.g., RandomForestRegressor)\n",
    "for the continuous 'Rank' score. Weâ€™ll do a simple example with a time-based split (older data\n",
    "as train, newer data as test). If you prefer a random split, you can just import train_test_split\n",
    "from sklearn and ignore the timestamps.\n",
    "\n",
    "Steps:\n",
    "1. Create a new DataFrame from df_encoded.\n",
    "2. Decide on a time-based cutoff (e.g., 80th percentile).\n",
    "3. Separate features vs. target (Rank).\n",
    "4. Train a RandomForestRegressor.\n",
    "5. Evaluate with MSE, MAE, Spearman correlation.\n",
    "6. Show how to do predictions.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# 1) We'll copy df_encoded into a new DataFrame\n",
    "model_df = df_encoded.copy()\n",
    "\n",
    "# 2) Time-based train/test split (80% oldest => train, 20% newest => test).\n",
    "cutoff_date = model_df[\"UpdateDT\"].quantile(0.80)  # or another approach\n",
    "train_df = model_df[model_df[\"UpdateDT\"] <= cutoff_date]\n",
    "test_df  = model_df[model_df[\"UpdateDT\"] >  cutoff_date]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size    : {len(test_df)}\")\n",
    "\n",
    "# 3) Define features & target\n",
    "# Exclude ID, date, and 'Rank' (since 'Rank' is the target we want to predict).\n",
    "exclude_cols2 = [\"SubjectID\", \"UpdateDT\", \"Rank\"]\n",
    "feature_cols2 = [col for col in model_df.columns if col not in exclude_cols2]\n",
    "\n",
    "X_train = train_df[feature_cols2]\n",
    "y_train = train_df[\"Rank\"]\n",
    "X_test  = test_df[feature_cols2]\n",
    "y_test  = test_df[\"Rank\"]\n",
    "\n",
    "# 4) (Optional) Scale features for the model\n",
    "# It's often beneficial to scale numeric columns. We can reuse StandardScaler if we want.\n",
    "scaler2 = StandardScaler()\n",
    "X_train_scaled = scaler2.fit_transform(X_train)\n",
    "X_test_scaled  = scaler2.transform(X_test)\n",
    "\n",
    "# 5) Initialize and train a RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6) Predictions & evaluation\n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "\n",
    "# Mean Squared Error, Mean Absolute Error, Spearman correlation\n",
    "mse_val = mean_squared_error(y_test, y_pred)\n",
    "mae_val = mean_absolute_error(y_test, y_pred)\n",
    "spearman_corr, _ = spearmanr(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- RandomForestRegressor Evaluation (Time-based Split) ---\")\n",
    "print(f\"Test MSE: {mse_val:.2f}\")\n",
    "print(f\"Test MAE: {mae_val:.2f}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr:.3f}\")\n",
    "\n",
    "# 7) Save the model for future usage\n",
    "with open(\"random_forest_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "print(\"\\nModel saved to 'random_forest_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "If you want to predict rank on new/unseen data, you must:\n",
    "1. Apply the same one-hot encoding to the new data (matching old factor columns).\n",
    "2. Scale using the same scaler (fit on training data).\n",
    "3. Load the trained model and call .predict().\n",
    "\n",
    "Below is a simplified example. If you have a 'new_data.csv' with the same columns,\n",
    "you would do something like:\n",
    "\"\"\"\n",
    "\n",
    "# Example demonstration (commented out):\n",
    "\"\"\"\n",
    "new_df = pd.read_csv('new_data.csv')\n",
    "new_df['UpdateDT'] = pd.to_datetime(new_df['UpdateDT'], errors='coerce')\n",
    "# Sort, etc., if needed\n",
    "\n",
    "# 1) One-hot encode the driver columns in the same way. You must replicate the exact\n",
    "#    dummy column structure. Usually, you'd re-use the same method or store the columns.\n",
    "#    For example:\n",
    "driver_cols_new = [c for c in new_df.columns if c.startswith('Driver')]\n",
    "new_df_encoded = pd.get_dummies(new_df, columns=driver_cols_new, drop_first=True)\n",
    "\n",
    "# Make sure new_df_encoded has the same columns as X_train did\n",
    "# If any columns are missing, you might need to add them with default 0. \n",
    "# If new columns appear, you should drop or handle them consistently.\n",
    "\n",
    "# 2) Scale using the same scaler\n",
    "X_new = new_df_encoded[feature_cols2]  # same feature_cols2 as before\n",
    "X_new_scaled = scaler2.transform(X_new)\n",
    "\n",
    "# 3) Load the model and predict\n",
    "with open('random_forest_model.pkl', 'rb') as f:\n",
    "    loaded_rf = pickle.load(f)\n",
    "\n",
    "new_preds = loaded_rf.predict(X_new_scaled)\n",
    "print(new_preds)\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
